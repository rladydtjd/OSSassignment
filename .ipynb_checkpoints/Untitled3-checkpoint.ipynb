{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24f73d72-c71d-43a8-9f79-77b1b0e28fc0",
   "metadata": {},
   "source": [
    "## ğŸ›œ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8498d408-5079-43c0-a64c-ae0c6f3f178f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e1d3724-4f20-4921-a27f-3811bfba42c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --- YTN ë©”ì¸ ë©”ë‰´ HTML ìŠ¤ë‹ˆí« (ì œê³µí•´ì£¼ì‹  ë‚´ìš©) ---\n",
    "# ì´ HTMLì„ íŒŒì‹±í•˜ì—¬ ì¹´í…Œê³ ë¦¬ ë§µì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "ytn_menu_html_snippet = \"\"\"\n",
    "                <ul class=\"menu\">\n",
    "\t\t\t\t\t<li class=\"YTN_CSA_mainpolitics menu_election2025\">\n",
    "\t\t\t\t\t\t<a href=\"https://www.ytn.co.kr/issue/election2025\">ëŒ€ì„ 2025</a>\n",
    "\t\t\t\t\t</li>\n",
    "\t\t\t\t\t<li class=\"YTN_CSA_mainpolitics \">\n",
    "\t\t\t\t\t\t<a href=\"https://www.ytn.co.kr/news/list.php?mcd=0101\">ì •ì¹˜</a>\n",
    "\t\t\t\t\t</li>\n",
    "\t\t\t\t\t<li class=\"YTN_CSA_maineconomy \">\n",
    "\t\t\t\t\t\t<a href=\"https://www.ytn.co.kr/news/list.php?mcd=0102\">ê²½ì œ</a>\n",
    "\t\t\t\t\t</li>\n",
    "\t\t\t\t\t<li class=\"YTN_CSA_mainsociety \">\n",
    "\t\t\t\t\t\t<a href=\"https://www.ytn\n",
    "                        .co.kr/news/list.php?mcd=0103\">ì‚¬íšŒ</a>\n",
    "\t\t\t\t\t</li>\n",
    "\t\t\t\t\t<li class=\"YTN_CSA_mainnationwide \">\n",
    "\t\t\t\t\t\t<a href=\"https://www.ytn.co.kr/news/list.php?mcd=0115\">ì „êµ­</a>\n",
    "\t\t\t\t\t</li>\n",
    "\t\t\t\t\t<li class=\"YTN_CSA_mainglobal \">\n",
    "\t\t\t\t\t\t<a href=\"https://www.ytn.co.kr/news/list.php?mcd=0104\">êµ­ì œ</a>\n",
    "\t\t\t\t\t</li>\n",
    "\t\t\t\t\t<li class=\"YTN_CSA_mainscience \">\n",
    "\t\t\t\t\t\t<a href=\"https://www.ytn.co.kr/news/list.php?mcd=0105\">ê³¼í•™</a>\n",
    "\t\t\t\t\t</li>\n",
    "\t\t\t\t\t<li class=\"YTN_CSA_mainculture \">\n",
    "\t\t\t\t\t\t<a href=\"https://www.ytn.co.kr/news/list.php?mcd=0106\">ë¬¸í™”</a>\n",
    "\t\t\t\t\t</li>\n",
    "\t\t\t\t\t<li class=\"YTN_CSA_mainsports \">\n",
    "\t\t\t\t\t\t<a href=\"https://www.ytn.co.kr/news/list.php?mcd=0107\">ìŠ¤í¬ì¸ </a>\n",
    "\t\t\t\t\t</li>\n",
    "\t\t\t\t\t<li class=\"YTN_CSA_mainphoto \">\n",
    "\t\t\t\t\t\t<a href=\"https://star.ytn.co.kr\">ì—°ì˜ˆ</a>\n",
    "\t\t\t\t\t</li>\n",
    "\t\t\t\t\t<li class=\"YTN_CSA_maingame \">\n",
    "\t\t\t\t\t\t<!--<a href=\"https://game.ytn.co.kr/news/list.php?mcd=0135\">ê²Œì„</a>-->\n",
    "\t\t\t\t\t\t<a href=\"https://game.ytn.co.kr\">ê²Œì„</a>\n",
    "\t\t\t\t\t</li>\n",
    "\t\t\t\t\t<li class=\"YTN_CSA_mainweather \">\n",
    "\t\t\t\t\t\t<a href=\"https://www.ytn.co.kr/weather/list_weather.php\">ë‚ ì”¨</a>\n",
    "\t\t\t\t\t</li>\n",
    "\t\t\t\t\t<li class=\"YTN_CSA_mainissue \">\n",
    "\t\t\t\t\t\t<a href=\"https://www.ytn.co.kr/news/main_issue.html\">ì´ìŠˆ</a>\n",
    "\t\t\t\t\t</li>\n",
    "\t\t\t\t\t<li class=\"YTN_CSA_mainyp \">\n",
    "\t\t\t\t\t\t<a href=\"https://www.ytn.co.kr/news/main_yp.html\">ì‹œë¦¬ì¦ˆ</a>\n",
    "\t\t\t\t\t</li>\n",
    "\t\t\t\t\t<li class=\"YTN_CSA_mainreplay \"><a href=\"https://www.ytn.co.kr/replay/main.html\">TVí”„ë¡œê·¸ë¨</a></li>\n",
    "\t\t\t\t</ul>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1dc7b105-506a-4f51-af90-568c7eb65877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ë©”ë‰´ HTML íŒŒì‹± ë° ì¹´í…Œê³ ë¦¬ ë§µ ìƒì„± ---\n",
    "ytn_menu_soup = BeautifulSoup(ytn_menu_html_snippet, 'html.parser')\n",
    "ytn_category_map = {} # ì¹´í…Œê³ ë¦¬ ë§µ ì´ˆê¸°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16948c74-8d77-4a4a-88fd-060154616d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ìƒì„±ëœ YTN ì¹´í…Œê³ ë¦¬ ë§µ ---\n",
      "{'0101': 'ì •ì¹˜', '0102': 'ê²½ì œ', '0103': 'ì‚¬íšŒ', '0115': 'ì „êµ­', '0104': 'êµ­ì œ', '0105': 'ê³¼í•™', '0106': 'ë¬¸í™”', '0107': 'ìŠ¤í¬ì¸ '}\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ë©”ë‰´ HTMLì—ì„œ a íƒœê·¸ë“¤ì„ ì°¾ìŠµë‹ˆë‹¤.\n",
    "menu_links = ytn_menu_soup.select('ul.menu a')\n",
    "\n",
    "for link in menu_links:\n",
    "    href = link.get('href')\n",
    "    text = link.get_text(strip=True)\n",
    "    if href and text:\n",
    "        parsed_url = urlparse(href)\n",
    "        # URL ê²½ë¡œê°€ '/news/list.php'ì´ê³  ì¿¼ë¦¬ ìŠ¤íŠ¸ë§ì— 'mcd' íŒŒë¼ë¯¸í„°ê°€ ìˆëŠ” ê²½ìš°\n",
    "        if parsed_url.path == '/news/list.php' and parsed_url.query:\n",
    "            query_params = parse_qs(parsed_url.query)\n",
    "            if 'mcd' in query_params and query_params['mcd'][0]:\n",
    "                mcd_code = query_params['mcd'][0]\n",
    "                ytn_category_map[mcd_code] = text # mcd ì½”ë“œë¥¼ í‚¤ë¡œ, ì¹´í…Œê³ ë¦¬ ì´ë¦„ì„ ê°’ìœ¼ë¡œ ì €ì¥\n",
    "                # print(f\"ë§µí•‘ ì¶”ê°€: {mcd_code} -> {text}\") # ë””ë²„ê¹…ìš© ì¶œë ¥\n",
    "\n",
    "# ìƒì„±ëœ ì¹´í…Œê³ ë¦¬ ë§µ í™•ì¸ (ì„ íƒ ì‚¬í•­)\n",
    "print(\"--- ìƒì„±ëœ YTN ì¹´í…Œê³ ë¦¬ ë§µ ---\")\n",
    "print(ytn_category_map)\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e91404df-8782-44ae-8efd-cb83aab8bd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_ytn_category_from_url(url, category_map):\n",
    "    \"\"\"\n",
    "    YTN ê¸°ì‚¬ URL ê²½ë¡œë¥¼ ë¶„ì„í•˜ì—¬ ì¹´í…Œê³ ë¦¬ ì½”ë“œë¥¼ ì¶”ì¶œí•˜ê³  ë§µí•‘ëœ ì¹´í…Œê³ ë¦¬ ì´ë¦„ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    ìƒì„±ëœ category_mapì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parsed_url = urlparse(url)\n",
    "        path = parsed_url.path # ì˜ˆ: '/_ln/0103_202505111017133914'\n",
    "        path_segments = path.split('/')\n",
    "        \n",
    "        if '_ln' in path_segments:\n",
    "            ln_index = path_segments.index('_ln')\n",
    "            if ln_index + 1 < len(path_segments):\n",
    "                # ì˜ˆ: '0103_202505111017133914'\n",
    "                code_segment = path_segments[ln_index + 1]\n",
    "                # ì½”ë“œ ì„¸ê·¸ë¨¼íŠ¸ì—ì„œ ì²« ë²ˆì§¸ '_' ì´ì „ ë¶€ë¶„ì´ ì¹´í…Œê³ ë¦¬ ì½”ë“œì…ë‹ˆë‹¤.\n",
    "                code = code_segment.split('_')[0] if '_' in code_segment else code_segment\n",
    "\n",
    "                # ìƒì„±ëœ category_mapì—ì„œ ì½”ë“œë¥¼ ì°¾ì•„ ì¹´í…Œê³ ë¦¬ ì´ë¦„ ë°˜í™˜\n",
    "                return category_map.get(code, f\"ì•Œ ìˆ˜ ì—†ëŠ” ì¹´í…Œê³ ë¦¬ ì½”ë“œ: {code}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"URL [{url}] ì¹´í…Œê³ ë¦¬ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "\n",
    "    # ì¼ì¹˜í•˜ëŠ” íŒ¨í„´ì„ ì°¾ì§€ ëª»í•˜ê±°ë‚˜ ì˜¤ë¥˜ ë°œìƒ ì‹œ\n",
    "    return \"ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ì‹¤íŒ¨ (URL íŒ¨í„´ ë¶ˆì¼ì¹˜)\"\n",
    "\n",
    "def get_ytn_article_data(url, headers, category_map):\n",
    "    \"\"\"\n",
    "    ë‹¨ì¼ YTN ê¸°ì‚¬ URLì—ì„œ ì œëª©, ë³¸ë¬¸, ì¹´í…Œê³ ë¦¬ë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜\n",
    "    ìƒì„±ëœ category_mapì„ ì¸ìë¡œ ë°›ìŠµë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    news_title = \"ì œëª© ì¶”ì¶œ ì‹¤íŒ¨\"\n",
    "    news_body = \"ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨\"\n",
    "    news_category = \"ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ ì‹¤íŒ¨\" # ì´ˆê¸° ì¹´í…Œê³ ë¦¬ ìƒíƒœ\n",
    "\n",
    "    print(f\"Processing URL: {url}\")\n",
    "\n",
    "    try:\n",
    "        # 1. ì›¹í˜ì´ì§€ HTML ê°€ì ¸ì˜¤ê¸°\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        html_content = response.text\n",
    "\n",
    "        # --- ë””ë²„ê¹…: ê°€ì ¸ì˜¨ HTMLì„ íŒŒì¼ë¡œ ì €ì¥ ---\n",
    "        file_name_safe = re.sub(r'[^\\w.-]', '_', urlparse(url).path.strip('/')).strip('_')\n",
    "        if not file_name_safe: file_name_safe = urlparse(url).hostname or 'debug'\n",
    "        debug_file_path = f\"debug_ytn_html_{file_name_safe}.html\"\n",
    "        try:\n",
    "            with open(debug_file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(html_content)\n",
    "            print(f\"ë””ë²„ê¹…: ê°€ì ¸ì˜¨ HTML ë‚´ìš©ì„ '{debug_file_path}' íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.\")\n",
    "        except Exception as file_error:\n",
    "            print(f\"ë””ë²„ê¹…: HTML íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {file_error}\")\n",
    "        # --- ë””ë²„ê¹… ë ---\n",
    "\n",
    "        # 2. BeautifulSoupìœ¼ë¡œ íŒŒì‹±\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # --- ë‰´ìŠ¤ ì œëª© ì¶”ì¶œ (ì œê³µí•´ì£¼ì‹  YTN êµ¬ì¡° ë°˜ì˜) ---\n",
    "        # h2 íƒœê·¸ì— class 'news_title'ë¥¼ ì°¾ê³ , ê·¸ ì•ˆì˜ span í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "        title_element_h2 = soup.find('h2', class_='news_title')\n",
    "\n",
    "        if title_element_h2:\n",
    "            title_element_span = title_element_h2.find('span')\n",
    "            if title_element_span:\n",
    "                news_title = title_element_span.get_text(strip=True)\n",
    "                print(f\"URL {url}: ì œëª© ìš”ì†Œ (h2.news_title > span) ì¶”ì¶œ ì„±ê³µ.\")\n",
    "            else:\n",
    "                 news_title = title_element_h2.get_text(strip=True) if title_element_h2.get_text(strip=True) else news_title\n",
    "                 print(f\"URL {url}: <h2 class='news_title'> íƒœê·¸ë¥¼ ì°¾ì•˜ìœ¼ë‚˜ <span>ì´ ì—†ì–´ <h2> í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„.\")\n",
    "        else:\n",
    "            print(f\"URL {url}: ì œëª© ìš”ì†Œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ì˜ˆìƒ ì„ íƒì: h2.news_title)\")\n",
    "\n",
    "\n",
    "        # --- ë‰´ìŠ¤ ë³¸ë¬¸ ì¶”ì¶œ (ì œê³µí•´ì£¼ì‹  div#CmAdContent.paragraph êµ¬ì¡° ë°˜ì˜) ---\n",
    "        # ë³¸ë¬¸ ì „ì²´ë¥¼ ê°ì‹¸ëŠ” div ìš”ì†Œë¥¼ ì°¾ìŠµë‹ˆë‹¤. idê°€ 'CmAdContent'ì´ê³  classê°€ 'paragraph'ì…ë‹ˆë‹¤.\n",
    "        body_container = soup.find('div', id='CmAdContent', class_='paragraph')\n",
    "        \n",
    "        news_body = \"ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨\"\n",
    "\n",
    "        if body_container:\n",
    "            # ë¶ˆí•„ìš”í•œ ìš”ì†Œ (ì˜ˆ: iframe ê´‘ê³ , ì´ë¯¸ì§€ ë“±) ì œê±°\n",
    "            for unnecessary_tag in body_container.find_all(['iframe', 'figure']):\n",
    "                unnecessary_tag.extract()\n",
    "\n",
    "            news_body_raw = body_container.get_text(separator='\\n', strip=True)\n",
    "\n",
    "            # ë¶ˆí•„ìš”í•œ ë‚´ìš© ì œê±° ë° ì •ë¦¬ (YTN ê¸°ì‚¬ í•˜ë‹¨ë¶€ íŒ¨í„´ ì œê±°)\n",
    "            cleaned_body = news_body_raw\n",
    "            cleaned_body = re.sub(r'YTN\\s*[^(\\n)]+\\s*\\([^@]+\\@[^)]+\\)\\s*\\n*', '', cleaned_body, flags=re.MULTILINE)\n",
    "            cleaned_body = re.sub(r'â€»\\s*.*?\\[ë©”ì¼\\].*?\\n*', '', cleaned_body, flags=re.DOTALL)\n",
    "            cleaned_body = re.sub(r'\\[ì €ì‘ê¶Œì\\(c\\).+?\\]\\n*', '', cleaned_body)\n",
    "\n",
    "            news_body = re.sub(r'\\n\\s*\\n', '\\n\\n', cleaned_body).strip()\n",
    "\n",
    "            if news_body:\n",
    "                print(f\"URL {url}: ë³¸ë¬¸ ìš”ì†Œ (div#CmAdContent.paragraph) ì¶”ì¶œ ì„±ê³µ.\")\n",
    "            else:\n",
    "                print(f\"URL {url}: ë³¸ë¬¸ ì»¨í…Œì´ë„ˆëŠ” ì°¾ì•˜ìœ¼ë‚˜, ìœ íš¨í•œ í…ìŠ¤íŠ¸ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤ (ì •ë¦¬ í›„ ë¹ˆ ë‚´ìš©).\")\n",
    "                news_body = \"ë³¸ë¬¸ ë‚´ìš© ì—†ìŒ\"\n",
    "\n",
    "        else:\n",
    "            print(f\"URL {url}: ë³¸ë¬¸ ì „ì²´ ì»¨í…Œì´ë„ˆ ìš”ì†Œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ì˜ˆìƒ ì„ íƒì: div#CmAdContent.paragraph)\")\n",
    "\n",
    "        # --- ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ (URL ê²½ë¡œ ë¶„ì„ - ìƒì„±ëœ ë§µ ì‚¬ìš©) ---\n",
    "        # ìƒì„±ëœ ytn_category_mapì„ classify_ytn_category_from_url í•¨ìˆ˜ì— ì „ë‹¬\n",
    "        news_category = classify_ytn_category_from_url(url, category_map)\n",
    "        if news_category == \"ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ì‹¤íŒ¨ (URL íŒ¨í„´ ë¶ˆì¼ì¹˜)\" or news_category.startswith(\"ì•Œ ìˆ˜ ì—†ëŠ” ì¹´í…Œê³ ë¦¬ ì½”ë“œ\"):\n",
    "             print(f\"URL {url}: URL êµ¬ì¡° ë¶„ì„ìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ/ë¶„ë¥˜ ì‹¤íŒ¨: {news_category}\")\n",
    "        else:\n",
    "             print(f\"URL {url}: URL êµ¬ì¡° ë¶„ì„ìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ '{news_category}' ì¶”ì¶œ ì„±ê³µ.\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"URL {url}: ì›¹í˜ì´ì§€ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        # ìš”ì²­ ì‹¤íŒ¨ ì‹œ ì œëª©, ë³¸ë¬¸, ì¹´í…Œê³ ë¦¬ëŠ” ì´ˆê¸° ì‹¤íŒ¨ ê°’ ìœ ì§€\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"URL {url}: ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}\")\n",
    "        # ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ì‹œ í•´ë‹¹ ê°’ë“¤ì€ ì´ˆê¸° ì‹¤íŒ¨ ê°’ ìœ ì§€\n",
    "        if news_category == \"ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ ì‹¤íŒ¨\": # ì˜¤ë¥˜ ë°œìƒí–ˆë”ë¼ë„ ì¹´í…Œê³ ë¦¬ë¼ë„ ì¶”ì¶œ ì‹œë„\n",
    "             news_category = classify_ytn_category_from_url(url, category_map) # ë§µì„ ì „ë‹¬\n",
    "\n",
    "    # ìµœì¢… ì¶”ì¶œ ê²°ê³¼ ë°˜í™˜\n",
    "    return {\n",
    "        'URL': url,\n",
    "        'ì œëª©': news_title,\n",
    "        'ë³¸ë¬¸': news_body,\n",
    "        'ì¹´í…Œê³ ë¦¬': news_category\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd7dd282-30f3-4d44-9f29-cb02a5266063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- í¬ë¡¤ë§í•  YTN ë‰´ìŠ¤ ê¸°ì‚¬ URL ëª©ë¡ì„ ì‘ì„±í•´ì£¼ì„¸ìš”. ---\n",
    "news_urls_to_crawl = [\n",
    "    'https://www.ytn.co.kr/_ln/0102_202505111058245968'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e6419b43-6150-47c2-94b8-1161c3525827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-Agent ì„¤ì •\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4910f8ab-dc6f-43c2-a171-cb3fdae886ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing URL: https://www.ytn.co.kr/_ln/0102_202505111058245968\n",
      "ë””ë²„ê¹…: ê°€ì ¸ì˜¨ HTML ë‚´ìš©ì„ 'debug_ytn_html_ln_0102_202505111058245968.html' íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.\n",
      "URL https://www.ytn.co.kr/_ln/0102_202505111058245968: ì œëª© ìš”ì†Œ (h2.news_title > span) ì¶”ì¶œ ì„±ê³µ.\n",
      "URL https://www.ytn.co.kr/_ln/0102_202505111058245968: ë³¸ë¬¸ ìš”ì†Œ (div#CmAdContent.paragraph) ì¶”ì¶œ ì„±ê³µ.\n",
      "URL https://www.ytn.co.kr/_ln/0102_202505111058245968: URL êµ¬ì¡° ë¶„ì„ìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ 'ê²½ì œ' ì¶”ì¶œ ì„±ê³µ.\n",
      "------------------------------\n",
      "\n",
      "ëª¨ë“  URL ì²˜ë¦¬ ì™„ë£Œ.\n"
     ]
    }
   ],
   "source": [
    "# ì¶”ì¶œëœ ë°ì´í„°ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "extracted_data_list = []\n",
    "\n",
    "# ê° URLì— ëŒ€í•´ í¬ë¡¤ë§ ë° ë°ì´í„° ì¶”ì¶œ ë°˜ë³µ\n",
    "for url in news_urls_to_crawl:\n",
    "    # ì¹´í…Œê³ ë¦¬ ë§µì„ get_ytn_article_data í•¨ìˆ˜ì— ì „ë‹¬\n",
    "    article_data = get_ytn_article_data(url, headers, ytn_category_map)\n",
    "    extracted_data_list.append(article_data)\n",
    "    print(\"-\" * 30) # êµ¬ë¶„ì„  ì¶œë ¥\n",
    "\n",
    "print(\"\\nëª¨ë“  URL ì²˜ë¦¬ ì™„ë£Œ.\")\n",
    "# --- ì¶”ì¶œëœ ë°ì´í„°ë¥¼ Pandas DataFrameìœ¼ë¡œ ë³€í™˜ ---\n",
    "df_news = pd.DataFrame(extracted_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "10e45711-157d-40c1-81ca-0e1f19ea6991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>ì œëª©</th>\n",
       "      <th>ë³¸ë¬¸</th>\n",
       "      <th>ì¹´í…Œê³ ë¦¬</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.ytn.co.kr/_ln/0102_202505111058245968</td>\n",
       "      <td>SKê·¸ë£¹, 'ì •ë³´ë³´í˜¸í˜ì‹ ìœ„ì›íšŒ' êµ¬ì„± ì‘ì—… ì°©ìˆ˜</td>\n",
       "      <td>SKê·¸ë£¹ ìµœíƒœì› íšŒì¥ì´ SKí…”ë ˆì½¤ í•´í‚¹ ì‚¬ê³  ì´í›„ ê·¸ë£¹ ë‚´ ë³´ì•ˆ ê°•í™” ëŒ€ì±…ìœ¼ë¡œ ë°œí‘œ...</td>\n",
       "      <td>ê²½ì œ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 URL  \\\n",
       "0  https://www.ytn.co.kr/_ln/0102_202505111058245968   \n",
       "\n",
       "                           ì œëª©  \\\n",
       "0  SKê·¸ë£¹, 'ì •ë³´ë³´í˜¸í˜ì‹ ìœ„ì›íšŒ' êµ¬ì„± ì‘ì—… ì°©ìˆ˜   \n",
       "\n",
       "                                                  ë³¸ë¬¸ ì¹´í…Œê³ ë¦¬  \n",
       "0  SKê·¸ë£¹ ìµœíƒœì› íšŒì¥ì´ SKí…”ë ˆì½¤ í•´í‚¹ ì‚¬ê³  ì´í›„ ê·¸ë£¹ ë‚´ ë³´ì•ˆ ê°•í™” ëŒ€ì±…ìœ¼ë¡œ ë°œí‘œ...   ê²½ì œ  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5f3f5fa3-7c20-4c57-927c-d9126a60e0f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    SKê·¸ë£¹ ìµœíƒœì› íšŒì¥ì´ SKí…”ë ˆì½¤ í•´í‚¹ ì‚¬ê³  ì´í›„ ê·¸ë£¹ ë‚´ ë³´ì•ˆ ê°•í™” ëŒ€ì±…ìœ¼ë¡œ ë°œí‘œ...\n",
       "Name: ë³¸ë¬¸, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news['ë³¸ë¬¸']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "617263fe-2c86-435f-a209-027ac3e258a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/janghongseo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'KoBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/janghongseo/nltk_data'\n    - '/Users/janghongseo/Main/Develop/OSSassignment/.venv/nltk_data'\n    - '/Users/janghongseo/Main/Develop/OSSassignment/.venv/share/nltk_data'\n    - '/Users/janghongseo/Main/Develop/OSSassignment/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# ì˜ˆì‹œ ì‚¬ìš©\u001b[39;00m\n\u001b[32m     39\u001b[39m text = df_news[\u001b[33m'\u001b[39m\u001b[33më³¸ë¬¸\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m summary = \u001b[43msummarize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mìš”ì•½ ê²°ê³¼:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m summary:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36msummarize\u001b[39m\u001b[34m(text, top_n)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msummarize\u001b[39m(text, top_n=\u001b[32m3\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     sentences = \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m     embeddings = [get_sentence_embedding(sent) \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences]\n\u001b[32m     26\u001b[39m     embeddings = np.array(embeddings)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Main/Develop/OSSassignment/.venv/lib/python3.13/site-packages/nltk/tokenize/__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Main/Develop/OSSassignment/.venv/lib/python3.13/site-packages/nltk/tokenize/__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Main/Develop/OSSassignment/.venv/lib/python3.13/site-packages/nltk/tokenize/punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Main/Develop/OSSassignment/.venv/lib/python3.13/site-packages/nltk/tokenize/punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Main/Develop/OSSassignment/.venv/lib/python3.13/site-packages/nltk/data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/janghongseo/nltk_data'\n    - '/Users/janghongseo/Main/Develop/OSSassignment/.venv/nltk_data'\n    - '/Users/janghongseo/Main/Develop/OSSassignment/.venv/share/nltk_data'\n    - '/Users/janghongseo/Main/Develop/OSSassignment/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "\n",
    "# ë¬¸ì¥ ë¶„ë¦¬\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "tokenizer = BertTokenizer.from_pretrained('monologg/kobert')\n",
    "model = BertModel.from_pretrained('monologg/kobert')\n",
    "model.eval()\n",
    "\n",
    "def get_sentence_embedding(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        # [CLS] í† í°ì˜ ë²¡í„° ì‚¬ìš©\n",
    "        return outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "\n",
    "import kss  # í•œêµ­ì–´ ë¬¸ì¥ ë¶„ë¦¬ê¸°\n",
    "\n",
    "def summarize(text, top_n=3):\n",
    "    sentences = kss.split_sentences(text)  # ì—¬ê¸°ë§Œ ë³€ê²½\n",
    "    embeddings = [get_sentence_embedding(sent) for sent in sentences]\n",
    "    embeddings = np.array(embeddings)\n",
    "\n",
    "    sim_matrix = cosine_similarity(embeddings, embeddings)\n",
    "    scores = sim_matrix.sum(axis=1)\n",
    "\n",
    "    ranked_sentences = [sent for _, sent in sorted(zip(scores, sentences), reverse=True)]\n",
    "    return ranked_sentences[:top_n]\n",
    "\n",
    "\n",
    "# ì˜ˆì‹œ ì‚¬ìš©\n",
    "text = df_news['ë³¸ë¬¸']\n",
    "summary = summarize(text)\n",
    "print(\"ìš”ì•½ ê²°ê³¼:\")\n",
    "for sent in summary:\n",
    "    print(\"-\", sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cf064c1a-4477-477f-b92d-4376d5e4478a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kss\n",
      "  Downloading kss-6.0.4.tar.gz (1.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting emoji==1.2.0 (from kss)\n",
      "  Downloading emoji-1.2.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting pecab (from kss)\n",
      "  Downloading pecab-1.0.8.tar.gz (26.4 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m26.4/26.4 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: networkx in ./.venv/lib/python3.13/site-packages (from kss) (3.4.2)\n",
      "Collecting jamo (from kss)\n",
      "  Downloading jamo-0.4.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting hangul-jamo (from kss)\n",
      "  Downloading hangul_jamo-1.0.1-py3-none-any.whl.metadata (899 bytes)\n",
      "Collecting tossi (from kss)\n",
      "  Downloading tossi-0.3.1.tar.gz (11 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting distance (from kss)\n",
      "  Downloading Distance-0.1.3.tar.gz (180 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pyyaml==6.0 (from kss)\n",
      "  Downloading PyYAML-6.0.tar.gz (124 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31mÃ—\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31mâ”‚\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31mâ•°â”€>\u001b[0m \u001b[31m[78 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m /private/var/folders/8v/lmh88sc94s904x9686hs_3n40000gn/T/pip-build-env-grajmsst/overlay/lib/python3.13/site-packages/setuptools/dist.py:761: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m         Please consider removing the following classifiers in favor of a SPDX license expression:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         License :: OSI Approved :: MIT License\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m   self._finalize_license_expression()\n",
      "  \u001b[31m   \u001b[0m running egg_info\n",
      "  \u001b[31m   \u001b[0m writing lib/PyYAML.egg-info/PKG-INFO\n",
      "  \u001b[31m   \u001b[0m writing dependency_links to lib/PyYAML.egg-info/dependency_links.txt\n",
      "  \u001b[31m   \u001b[0m writing top-level names to lib/PyYAML.egg-info/top_level.txt\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/Users/janghongseo/Main/Develop/OSSassignment/.venv/lib/python3.13/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\"\u001b[0m, line \u001b[35m389\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/Users/janghongseo/Main/Develop/OSSassignment/.venv/lib/python3.13/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\"\u001b[0m, line \u001b[35m373\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     json_out[\"return_val\"] = \u001b[31mhook\u001b[0m\u001b[1;31m(**hook_input[\"kwargs\"])\u001b[0m\n",
      "  \u001b[31m   \u001b[0m                              \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/Users/janghongseo/Main/Develop/OSSassignment/.venv/lib/python3.13/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\"\u001b[0m, line \u001b[35m143\u001b[0m, in \u001b[35mget_requires_for_build_wheel\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     return hook(config_settings)\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/private/var/folders/8v/lmh88sc94s904x9686hs_3n40000gn/T/pip-build-env-grajmsst/overlay/lib/python3.13/site-packages/setuptools/build_meta.py\"\u001b[0m, line \u001b[35m331\u001b[0m, in \u001b[35mget_requires_for_build_wheel\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     return \u001b[31mself._get_build_requires\u001b[0m\u001b[1;31m(config_settings, requirements=[])\u001b[0m\n",
      "  \u001b[31m   \u001b[0m            \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/private/var/folders/8v/lmh88sc94s904x9686hs_3n40000gn/T/pip-build-env-grajmsst/overlay/lib/python3.13/site-packages/setuptools/build_meta.py\"\u001b[0m, line \u001b[35m301\u001b[0m, in \u001b[35m_get_build_requires\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31mself.run_setup\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/private/var/folders/8v/lmh88sc94s904x9686hs_3n40000gn/T/pip-build-env-grajmsst/overlay/lib/python3.13/site-packages/setuptools/build_meta.py\"\u001b[0m, line \u001b[35m317\u001b[0m, in \u001b[35mrun_setup\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31mexec\u001b[0m\u001b[1;31m(code, locals())\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m288\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/private/var/folders/8v/lmh88sc94s904x9686hs_3n40000gn/T/pip-build-env-grajmsst/overlay/lib/python3.13/site-packages/setuptools/__init__.py\"\u001b[0m, line \u001b[35m117\u001b[0m, in \u001b[35msetup\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     return \u001b[31mdistutils.core.setup\u001b[0m\u001b[1;31m(**attrs)\u001b[0m\n",
      "  \u001b[31m   \u001b[0m            \u001b[31m~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/private/var/folders/8v/lmh88sc94s904x9686hs_3n40000gn/T/pip-build-env-grajmsst/overlay/lib/python3.13/site-packages/setuptools/_distutils/core.py\"\u001b[0m, line \u001b[35m186\u001b[0m, in \u001b[35msetup\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     return run_commands(dist)\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/private/var/folders/8v/lmh88sc94s904x9686hs_3n40000gn/T/pip-build-env-grajmsst/overlay/lib/python3.13/site-packages/setuptools/_distutils/core.py\"\u001b[0m, line \u001b[35m202\u001b[0m, in \u001b[35mrun_commands\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31mdist.run_commands\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/private/var/folders/8v/lmh88sc94s904x9686hs_3n40000gn/T/pip-build-env-grajmsst/overlay/lib/python3.13/site-packages/setuptools/_distutils/dist.py\"\u001b[0m, line \u001b[35m1002\u001b[0m, in \u001b[35mrun_commands\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31mself.run_command\u001b[0m\u001b[1;31m(cmd)\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/private/var/folders/8v/lmh88sc94s904x9686hs_3n40000gn/T/pip-build-env-grajmsst/overlay/lib/python3.13/site-packages/setuptools/dist.py\"\u001b[0m, line \u001b[35m1106\u001b[0m, in \u001b[35mrun_command\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31msuper().run_command\u001b[0m\u001b[1;31m(command)\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/private/var/folders/8v/lmh88sc94s904x9686hs_3n40000gn/T/pip-build-env-grajmsst/overlay/lib/python3.13/site-packages/setuptools/_distutils/dist.py\"\u001b[0m, line \u001b[35m1021\u001b[0m, in \u001b[35mrun_command\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31mcmd_obj.run\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/private/var/folders/8v/lmh88sc94s904x9686hs_3n40000gn/T/pip-build-env-grajmsst/overlay/lib/python3.13/site-packages/setuptools/command/egg_info.py\"\u001b[0m, line \u001b[35m312\u001b[0m, in \u001b[35mrun\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31mself.find_sources\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/private/var/folders/8v/lmh88sc94s904x9686hs_3n40000gn/T/pip-build-env-grajmsst/overlay/lib/python3.13/site-packages/setuptools/command/egg_info.py\"\u001b[0m, line \u001b[35m320\u001b[0m, in \u001b[35mfind_sources\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31mmm.run\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/private/var/folders/8v/lmh88sc94s904x9686hs_3n40000gn/T/pip-build-env-grajmsst/overlay/lib/python3.13/site-packages/setuptools/command/egg_info.py\"\u001b[0m, line \u001b[35m543\u001b[0m, in \u001b[35mrun\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31mself.add_defaults\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/private/var/folders/8v/lmh88sc94s904x9686hs_3n40000gn/T/pip-build-env-grajmsst/overlay/lib/python3.13/site-packages/setuptools/command/egg_info.py\"\u001b[0m, line \u001b[35m581\u001b[0m, in \u001b[35madd_defaults\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31msdist.add_defaults\u001b[0m\u001b[1;31m(self)\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/private/var/folders/8v/lmh88sc94s904x9686hs_3n40000gn/T/pip-build-env-grajmsst/overlay/lib/python3.13/site-packages/setuptools/command/sdist.py\"\u001b[0m, line \u001b[35m109\u001b[0m, in \u001b[35madd_defaults\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31msuper().add_defaults\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/private/var/folders/8v/lmh88sc94s904x9686hs_3n40000gn/T/pip-build-env-grajmsst/overlay/lib/python3.13/site-packages/setuptools/_distutils/command/sdist.py\"\u001b[0m, line \u001b[35m245\u001b[0m, in \u001b[35madd_defaults\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31mself._add_defaults_ext\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     \u001b[31m~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/private/var/folders/8v/lmh88sc94s904x9686hs_3n40000gn/T/pip-build-env-grajmsst/overlay/lib/python3.13/site-packages/setuptools/_distutils/command/sdist.py\"\u001b[0m, line \u001b[35m330\u001b[0m, in \u001b[35m_add_defaults_ext\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     self.filelist.extend(\u001b[31mbuild_ext.get_source_files\u001b[0m\u001b[1;31m()\u001b[0m)\n",
      "  \u001b[31m   \u001b[0m                          \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m204\u001b[0m, in \u001b[35mget_source_files\u001b[0m\n",
      "  \u001b[31m   \u001b[0m   File \u001b[35m\"/private/var/folders/8v/lmh88sc94s904x9686hs_3n40000gn/T/pip-build-env-grajmsst/overlay/lib/python3.13/site-packages/setuptools/_distutils/cmd.py\"\u001b[0m, line \u001b[35m131\u001b[0m, in \u001b[35m__getattr__\u001b[0m\n",
      "  \u001b[31m   \u001b[0m     raise AttributeError(attr)\n",
      "  \u001b[31m   \u001b[0m \u001b[1;35mAttributeError\u001b[0m: \u001b[35mcython_sources\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31mÃ—\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "\u001b[31mâ”‚\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "\u001b[31mâ•°â”€>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8564f13c-7c25-4bd3-bb56-f03179ffae5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
